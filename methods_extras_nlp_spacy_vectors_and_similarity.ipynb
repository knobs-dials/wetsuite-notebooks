{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities\n",
    "Most models add vectors to each token (tok2vec, or transformer-based for the _trf model),\n",
    "which are word embeddings, so should compare well for words with similar meaning.\n",
    "\n",
    "This assists tasks like calculating similarity of larger chunks of text as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "english_lg  = spacy.load('en_core_web_lg')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.869                                      ducks are great                                      cats are nice\n",
      "0.872                                      ducks are great                                     goats are cool\n",
      "0.796                                      ducks are great                                   Forks are spoons\n",
      "0.383                                      ducks are great                                   Forks and spoons\n",
      "0.450                                      ducks are great                        Forks and spoons and knives\n",
      "0.761                                      ducks are great                        Forks and spoons are knives\n",
      "0.868                     ducks and blah and blah and blah                    blue and bleh and bleh and bleh\n",
      "0.218                                                ducks                                               blue\n",
      "\n",
      "0.732\n",
      "  [Because it is smaller, the Moon has less gravity than Earth (only 1/6 of the amount on Earth).]\n",
      "  [So if a person weighs 60 kilograms on Earth, the person would only weigh 10 kilograms on the moon.]\n",
      "0.688\n",
      "  [So if a person weighs 60 kilograms on Earth, the person would only weigh 10 kilograms on the moon.]\n",
      "  [But even though the Moon's gravity is weaker than the Earth's gravity, it is still there.]\n",
      "0.764\n",
      "  [But even though the Moon's gravity is weaker than the Earth's gravity, it is still there.]\n",
      "  [If a person dropped a ball while standing on the moon, it would still fall down.]\n",
      "0.803\n",
      "  [If a person dropped a ball while standing on the moon, it would still fall down.]\n",
      "  [However, it would fall much more slowly.]\n",
      "0.763\n",
      "  [However, it would fall much more slowly.]\n",
      "  [A person who jumped as high as possible on the moon would jump higher than on Earth, but still fall back to the ground.]\n",
      "0.815\n",
      "  [A person who jumped as high as possible on the moon would jump higher than on Earth, but still fall back to the ground.]\n",
      "  [Rome ceased to be the capital from the time of the division.]\n",
      "0.687\n",
      "  [Rome ceased to be the capital from the time of the division.]\n",
      "  [In 286, the capital of the Western Roman Empire became Mediolanum (now Milan).]\n",
      "0.737\n",
      "  [In 286, the capital of the Western Roman Empire became Mediolanum (now Milan).]\n",
      "  [In 402, the capital was again moved, this time to Ravenna.]\n",
      "0.911\n",
      "  [In 402, the capital was again moved, this time to Ravenna.]\n",
      "  [In AD 398, Alaric led the Visigoths and began making attacks closer and closer to the capital.]\n",
      "0.743\n",
      "  [In AD 398, Alaric led the Visigoths and began making attacks closer and closer to the capital.]\n",
      "  [By 410, he had sacked the Rome.]\n",
      "0.752\n",
      "  [By 410, he had sacked the Rome.]\n",
      "  [In 455, the Vandals captured the city.]\n",
      "0.973\n",
      "  [In 455, the Vandals captured the city.]\n",
      "  [In 476, the Goths captured the capital]\n"
     ]
    }
   ],
   "source": [
    "# You can generally compare any token/span (spans/sentence/docs will act as their average) using .similarity(). \n",
    "# \n",
    "# There are some fundamental limitations to this to keep in mind, like \n",
    "# - that it does not consider ordering, just words' presence\n",
    "# - how volatile the meaning of short sentences may be\n",
    "# - how function words dilute larger-span vectors (and might make them compare well for non-contentful reasons)\n",
    "# - 'static vectors' basically means a word has the same vector in all contexts.\n",
    "\n",
    "for one, other in (\n",
    "        ('ducks are great', 'cats are nice'),\n",
    "        ('ducks are great', 'goats are cool'),\n",
    "        ('ducks are great', 'Forks are spoons'),\n",
    "        ('ducks are great', 'Forks and spoons'),\n",
    "        ('ducks are great', 'Forks and spoons and knives'),\n",
    "        ('ducks are great', 'Forks and spoons are knives'),\n",
    "        ('ducks and blah and blah and blah', 'blue and bleh and bleh and bleh'),\n",
    "        ('ducks',           'blue'),\n",
    "    ):\n",
    "    sim = english_lg( one ).similarity(english_lg( other ))\n",
    "    print( \"%.3f   %50s %50s\"%( sim, one, other ))\n",
    "\n",
    "# These three-world phrases are actually quite contrived - real sentences have a narrower range of \n",
    "\n",
    "print()\n",
    "# these are sentences from two different wikipedia articles; we are trying to see if it can tell a difference.\n",
    "text = \"\"\"Because it is smaller, the Moon has less gravity than Earth (only 1/6 of the amount on Earth). \n",
    "So if a person weighs 60 kilograms on Earth, the person would only weigh 10 kilograms on the moon. \n",
    "But even though the Moon's gravity is weaker than the Earth's gravity, it is still there. \n",
    "If a person dropped a ball while standing on the moon, it would still fall down. However, it would fall much more slowly.\n",
    "A person who jumped as high as possible on the moon would jump higher than on Earth, but still fall back to the ground.\n",
    "Rome ceased to be the capital from the time of the division. \n",
    "In 286, the capital of the Western Roman Empire became Mediolanum (now Milan). \n",
    "In 402, the capital was again moved, this time to Ravenna.\n",
    "In AD 398, Alaric led the Visigoths and began making attacks closer and closer to the capital.\n",
    "By 410, he had sacked the Rome. In 455, the Vandals captured the city. In 476, the Goths captured the capital \"\"\"\n",
    "\n",
    "doc = english_lg( text )\n",
    "sents = list(doc.sents)\n",
    "for i in range(len(sents)-1):\n",
    "    one, other = sents[i], sents[i+1] \n",
    "    sim = one.similarity( other )\n",
    "    print( \"%.3f\\n  [%s]\\n  [%s]\"%( sim, one.text.strip(), other.text.strip() ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Spacy has, however, made things a bit complex.\n",
    "- _sm models don't have vectors.  While similarity() still does something useful at all, you should assume this is extremely basic.\n",
    "- _md and _lg  models tend to have **static word vectors**, for english it may be GroVe vectors, great in itself _but_ a word will receive the same vector in all contexts\n",
    "- _trf do context sensitive embeddings (and put those values in a different place)\n",
    "\n",
    "This means that\n",
    "* spacy's similarity() call does different things in difference models\n",
    "* you can't always pick out the vectors directly (though you can often get away with it if you stick to one model)\n",
    "\n",
    "\n",
    "Note that\n",
    "- scores on spans and docs act as the average of their compobnents\n",
    "- ...which also means e.g. function words can dilute larger-span vectors (and might make them compare well for non-contentful reasons)\n",
    "- (...so...) similarity() does not consider ordering, just words' presence\n",
    "- shorter sentences have minimal and more volative meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install spacy\n",
    "#!python3 -m spacy download en_core_web_trf   # works better, but can be rather slow without GPU properly set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "english_trf = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while _trf can do contextual word embedding rather than static word embedding, this is not placed in .vector.tensors\n",
    "# You could fish out the tensors like   toks_vectors, doc_vector = doc._.trf_data.tensors\n",
    "#   but it's handier to augent spacy to make similarity work with transformer tensors - custom pipeline element defined in our helper module\n",
    "\n",
    "# this mentioned tensor2attr is not basic spacy, it exists because it is defined in our helpers_spacy\n",
    "if not english_trf.has_pipe('tensor2attr'):\n",
    "    print(\"adding transformer based similarity\")\n",
    "    english_trf.add_pipe('tensor2attr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy.tokens\n",
    "spacy.tokens.Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = english_trf(\"The bank stores investment capital in Paris, the capital of France\")\n",
    "first_capital, second_capital = doc[4], doc[9]\n",
    "print( first_capital, second_capital, round( first_capital.similarity(second_capital), 2) ) \n",
    "# without contextual word embeddings, both capitals would be the same, and their similarity 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out what word it's close to\n",
    "money  = english_trf(\"money\")[0]\n",
    "city   = english_trf(\"city\")[0]\n",
    "paris  = english_trf(\"lyon\")[0]\n",
    "\n",
    "print ('              ','money', 'city', 'lyon')\n",
    "for in_sent, example in ( (' bank_capital ',first), ('city_capital',second) ):\n",
    "    print( '%15s %4.2f %4.2f %4.2f'%(in_sent, round(example.similarity(money), 2), round( example.similarity(city), 2), round( example.similarity(paris), 2)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "#vocablist = list(english_lg.vocab.strings)\n",
    "vvv = []\n",
    "i=0\n",
    "for string in list(english_lg.vocab.strings)[::10]:\n",
    "    if len(string)<4:\n",
    "        continue\n",
    "    n = english_lg(string)[0].norm\n",
    "    if n > 800000000000000000:\n",
    "        continue\n",
    "\n",
    "    print( n, string, english_lg.vocab.strings[string])\n",
    "    \n",
    "    if string in english_lg.vocab.vectors:\n",
    "        print( string, english_lg.vocab.strings[string], numpy.abs(english_lg.vocab.vectors[string]))\n",
    "    \n",
    "    vvv.append( string )\n",
    "\n",
    "    i+=1\n",
    "    if i>1000:\n",
    "        break\n",
    "print(i)\n",
    "print(len(vvv))\n",
    "\n",
    "vocablist = vvv\n",
    "\n",
    "#english_lg.vocab.lookups\n",
    "#for string in \n",
    "#print(len(vocablist))\n",
    "#vocablist[0].prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english_md = spacy.load(\"en_core_web_md\")   \n",
    "\n",
    "allsim1, allsim2 = {}, {}\n",
    "#vocablist = list(english_lg.vocab.strings)\n",
    "\n",
    "print( len(vocablist) )\n",
    "\n",
    "for word in vocablist[::10]:\n",
    "    isolated = english_trf(word)\n",
    "    allsim1[word] = first_capital.similarity( isolated )\n",
    "    allsim2[word] = second_capital.similarity( isolated )\n",
    "\n",
    "allsim1 = list(allsim1.items())\n",
    "allsim1.sort(key = lambda x:x[1], reverse=True)\n",
    "print( allsim1[:10] )\n",
    "\n",
    "allsim2 = list(allsim2.items())\n",
    "allsim2.sort(key = lambda x:x[1], reverse=True)\n",
    "print( allsim2[:10] )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things like 'find similar words within texts' will rely on some variant of 'compare everything to everything'\n",
    "# I've not found a spacy way to do such mass comparisons other than to call .similarity() a lot, which is a bunch of overhead\n",
    "# Since it seems to just be cosine similarity, we can use scipy to do a lot more comparisons in one go - code for which is in our helper\n",
    "\n",
    "print( \"SENTENCE SIMILARITY\" )\n",
    "for score, one, two in helpers_spacy.similar_sentences(doc,     thresh=0.5, n=5):   # yes, these these thresholds are chosen to give good results with this example. Play with them to see how messy it actually is.\n",
    "    print( \"    %5.2f  %40r  %40r\"%(score, one, two) )\n",
    "    \n",
    "print( \"TOKEN SIMILARITY\" )\n",
    "for score, one, two in helpers_spacy.similar_chunks(doc, 1,0,0, thresh=0.6, n=5):\n",
    "    print( \"    %5.2f  %40s  %40s\"%(score, one, two) )\n",
    "\n",
    "print( \"ENTITY AND NOUN CHUNK SIMILARITY\" )\n",
    "for score, one, two in helpers_spacy.similar_chunks(doc, 0,1,1, thresh=0.7, n=5):\n",
    "    print( \"    %5.2f  %40s  %40s\"%(score, one, two) )\n",
    "\n",
    "# It's generally not so useful to compare tokens with phrases from the same document, in that the top similarities will be phrases with their own head/root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the average of a sentence or document would be a lot of function words, \n",
    "#   direct comparison would still work but be watered down depending on how many of those there are\n",
    "\n",
    "\n",
    "\n",
    "#   so you might like \n",
    "# At the same time, spacy prefers its parsed object immutable, so you would have to work around it\n",
    "import numpy\n",
    "from importlib import reload\n",
    "reload(helpers_spacy)\n",
    "for sent in paris.sents:\n",
    "    print( '-'*80 )\n",
    "    print( sent )\n",
    "    sg = helpers_spacy.interesting_words( sent )\n",
    "    print( sg )\n",
    "    vpt = helpers_spacy.vector_per_tag(sent, average=True) \n",
    "    for tag, ary in vpt.items():\n",
    "        print( tag, numpy.linalg.norm(ary))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
